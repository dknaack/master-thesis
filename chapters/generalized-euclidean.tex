\chapter{The Generalized Euclidean Algorithm}

For a real number, we used the Euclidean algorithm to construct its continued
fraction.
We saw that a continued fractions is periodic if and only if the number used in
its construction is a quadratic irrational.
Naturally, we can ask for a generalization of the Euclidean algorithm,
which can be used to construct higher-dimensional continued fractions
and potentially leads to an answer of Hermite's question.

In this chapter, we will analyze one such possible candidate,
which is a generalization of the Euclidean algorithm by Klein and Reuter~\cite{Klein24}.
While the Euclidean algorithm works with integers,
the generalized version works with vectors.
It solves the problem of lattice basis computation,
which can be seen as a multi-dimensional analogue of computing the greatest
common divisor.

% ==============================================================================
\section{Basics of Lattice Theory}
% ==============================================================================

One of the fundamental structures in linear algebra is the vector space.
Given a set of vectors $A = \{A_1, …, A_n\} ∈ ℝ^d$, the span of $A$ is the set
of all linear combinations of the basis vectors $A_i$.
A lattice is similarly defined over a set of vectors $A$, but instead of linear
combinations of vectors, a lattice consists of only integral linear
combinations.
Formally, a lattice is defined as follows:

\begin{definition}
  Given a matrix $A ∈ ℤ^{d × n}$, the \emph{lattice} over the basis $B$ is defined as
  \[
    \mathcal{L}(A) = \left\{\, A₁z₁ + \dots + A_n z_n \mid z_1, \dots, z_n ∈ ℤ^d \,\right\}.
  \]
  The \emph{rank} of $\mathcal{L}(A)$ is $n$ and its \emph{dimension} is $d$.
  If $n = d$, then $\mathcal{L}(A)$ is a \emph{full rank} lattice and $A$ is a \emph{basis}.
\end{definition}

In a vector space, an over-defined set of vectors $A = \{A₁, …, Aₙ\}$ with $n > d$
can be easily reduced to a basis $B$ with exactly $d$ vectors by taking $d$
linearly independent vectors from the set $A$.
In a lattice, it is not as simple.
Taking only $d$ linearly independent vectors produces a different lattice than
the whole set, in general.
For example, consider the matrix
\[
  A = \begin{pmatrix}
    3 & 0 & 2 \\
    0 & 2 & 1 \\
  \end{pmatrix}.
\]
Taking any two columns $A_i, A_j$ of the matrix produces a linearly independent
set, but no set produces the same lattice as the whole matrix.
The missing vector $A_k$ cannot be reached by an integral combination of the
other two vectors $A_i, A_j$.

% TODO: Shouldn't it be the rank instead of n > d?
The problem of finding a basis for a lattice is typically known as \emph{lattice basis computation}.
For this problem, the input is a matrix $A ∈ ℤ^{d×n}$ with $n > d$ and the goal
is to find a basis which produces the same lattice, i.e. $\mathcal L(A) = \mathcal L(B)$.
The problem can also be seen as a multi-dimensional analogue of
computing the greatest common divisor of a set of numbers.
Indeed, we can view a set of numbers $a₁, a₂, …, aₙ ∈ ℤ$ as the basis vectors
of the one-dimensional lattice
\[
  a₁ℤ + a₂ ℤ + \dots + aₙ ℤ.
\]
The greatest common divisor represents a reduced basis in this lattice.

Previous approaches for computing such bases either used a variant of the LLL
algorithm~\cite{Lenstra82} or computed the Hermite normal form of the
matrix~\cite{Storjohann96}.
The generalized Euclidean algorithm uses a fundamentally different approach.

\begin{figure}[tbp]
  \centering
  \includestandalone{figures/lattice}
  \caption{
    A two-dimensional lattice $\mathcal L(B)$ with the basis vectors $B_1 = (2,
    1)$ and $B_2 = (1, 3)$. The fundamental parallelepiped $Π(B)$ is colored in
    {\color{cyan}cyan}.
  }
\end{figure}

\begin{itemize}
  \item Applications? \cite{Ajtai96,Gentry08}
\end{itemize}

% ==============================================================================
\section{Description of the Algorithm}
% ==============================================================================

\begin{Pseudocode}[
    float=tb,
    label={lst:generalized-euclidean},
    caption={The Generalized Euclidean Algorithm \cite{Klein24}.}]
solve $Bx = c$
while $x$ is not integral do
  find $x_ℓ$ which is not integral
  $c ← B_ℓ$
  $B_ℓ ← B\{x\}$
  solve $Bx = c$
end
\end{Pseudocode}

The algorithm is shown in Listing~\ref{lst:generalized-euclidean}.
It takes a matrix $B ∈ ℤ^{d × d}$ and a vector $c ∈ ℤ^d$ as input and finds a
new basis $B'$ such that $\mathcal L(B) = \mathcal L(B ∪ \{c\})$.
Its basic structure consists of two steps:
A modulo operation and an exchange operation.
The operations are similar to the Euclidean algorithm, where we first compute
the modulo $a \bmod b$ and then swap the two inputs.

First, we look at the modulo operation.
Suppose we have a basis $B$ with vectors $B₁, …, B_d$, any vector $a ∈ ℝ^d$ can
easily be represented as a linear combination $B₁x₁ + \dots + B_d x_d$ of the
basis vectors.
However, linear combinations are not integral combinations,
so the vector $a$ might not be in the lattice $\mathcal L(B)$.
But the vector $z = B₁ \floor{x₁} + \dots + B_d \floor{x_d}$
is a lattice point.
We could say that $z = B\floor{x}$ is the integer part of the vector.
The fractional part of $a$ would then be $r = a - z$.
The fractional part is actually contained in what is known as the \emph{fundamental parallelepiped} of the lattice.

\begin{definition}
  The \emph{fundamental parallelepiped} of a lattice $\mathcal{L}(B)$ with $B ∈ ℤ^{d × n}$ is defined as
  \[
    Π(B) = \left\{\, B₁ x₁ + \dots + B_n x_n \mid x_1, \dots, x_n ∈ [0, 1) \,\right\}
  \]
  The volume of the parallelepiped is
  \[
    \mathrm{vol}(Π(B)) = |\det(B)|.
  \]
\end{definition}

\iffalse
A useful fact about the fundamental parallelepiped of a lattice $\mathcal L(B)$
is that if $B$ is a square integer matrix, then the volume of the
parallelepiped $Π(B)$ and the number of integer points $ℤ^n$ contained in
$Π(B)$ is entirely determined by $\mathrm{det}(B)$, i.e.
\[
  \mathrm{vol}(Π(B)) = |Π(B) ∩ ℤ^n| = |\det(B)|.
\]
\fi

So in a lattice, each point $a ∈ ℝ^d$ can be represented as a combination of a lattice point $z
∈ \mathcal{L}(B)$ and a point in the fundamental parallelepiped $r ∈ Π(B)$.
Specifically,
\[
  a = z + r.
\]
This is essentially a multi-dimensional division with remainder operation inside a lattice.
It allows us to define a modulo operation for the algorithm as follows:
\[
  a \pmod{Π(B)} := a - B\floor{B^{-1} a}.
\]
It remains to show that applying this modulo operation on $a$ does not affect
the underlying lattice $\mathcal L(B ∪ \{a\})$.

\begin{lemma}
  Given a set of vectors $A = \{A_1, \dots, A_n\}$ with $A_i ∈ ℤ^d$
  and a linearly independent subset $B$,
  let $r = c \pmod{Π(B)}$ for any vector $c ∈ A \setminus B$.
  Then,
  \[
    \mathcal L(A) = \mathcal L(A \setminus \{c\} ∪ \{r\}).
  \]
\end{lemma}

\begin{proof}
  By definition of the modulo operation $c'$ is an integral combination of $c'$ and $B$,
  specifically $c' = c - B \floor{B^{-1} c}$.
  Therefore, $c' ∈ \mathcal L(A)$ and $\mathcal L(A) ⊆ \mathcal L(A \setminus \{c\} ∪ \{c'\})$.
  Similarly, we can reach $c$ by the integral combination $c' + B \floor{B^{-1} c}$,
  so $c ∈ \mathcal L(A \setminus \{c\} ∪ \{c'\})$.
  Therefore applying the modulo operation does not change the lattice.
\end{proof}

With the modulo operation done, we turn our attention to the exchange operation.
On one side we have the basis $B$ and on the other we have the vector $c$.
So a swap of the two inputs will not work.
Instead, we swap $c$ with a single column $B_ℓ$ of the basis $B$,
where the specific index $ℓ$ depends on the result from the modulo operation.
For the modulo we solve a linear system $Bx = c$.
If one element $x_i$ is integral, then $c$ is already contained in the
sub-lattice $\mathcal L(B_i)$.
In the one-dimensional case, this means that $c$ is a multiple of $B_i$,
so it wouldn't make sense to swap with $B_i$.
Instead, we only exchange $c$ with a vector $B_ℓ$, where $x_ℓ$ is not integral
and we swap $B_ℓ$ with the remainder $c \pmod{Π(B)}$.

The exchange operation also leads directly to the stop condition.
The algorithm terminates when all the vector $x$ is entirely integral.
In this case, the vector $c$ is an integral combination of the basis vectors,
so we can leave it out of the basis.

\begin{lemma}
  The algorithm terminates in at most $\det(B)$ iterations.
\end{lemma}

\begin{proof}
  By Cramer's rule, $\{x_ℓ\} = \frac{\det B'}{\det B}$
  and therefore the determinant decreases by a factor of $\{x_ℓ\} < 1$.
  Since the vectors in $B$ and $B'$ are integral, the determinant must also be integral.
  Therefore, the determinant decreases by at least one and in total it
  decreases at most $\det(B)$ iterations.
\end{proof}

In summary, the algorithm requires the following steps:
\begin{itemize}
  \item \textbf{Modulo}: $c \pmod{\Pi(B)} = c - B \floor{B^{-1} c}$.
  \item \textbf{Exchange}: $c = B_ℓ$
  \item \textbf{Termination}: $B^{-1} c ∈ ℤ^d$
\end{itemize}
and the previous two lemmas lead to the following theorem:

\begin{theorem}
  The generalized Euclidean algorithm solves the lattice basis reduction problem.
\end{theorem}

% ==============================================================================
\section{Extension to Real Numbers}
% ==============================================================================

% TODO: There should be some explanation on the fact that the solution vector
% kind of represents the ratio between the inputs just like in the Euclidean
% algorithm.
The last step needed for the analysis is an extension of the algorithm to real
numbers.
In the original Euclidean algorithm,
we used the ratio $b^{-1}a$ between the two inputs to derive a variant,
which works on real numbers.
In a sense, we use the solution vector $x = B^{-1}a$ as a ratio between the inputs
and we allow $x$ to be a real vector.
In the original lattice reduction problem, this means that we allow the
algorithm to take in real vectors as input.
Just like with the Euclidean algorithm, this causes the algorithm to continue
infinitely as long as the vector $x$ is not rational.

One problem is that calculating the next solution vector requires solving a
linear system.
However, there is a more direct way to calculate the solution vector $x$.
In the original paper, the authors proceed to apply various optimization
to speed up the basic algorithm presented here.
One of those optimizations, removes the need to solve a linear system in each iteration.
Instead of recomputing a solution each time,
we can update the current solution to gain the solution for the next iteration.
If $x = (x₁, …, x_d)$ is the solution in the current iteration,
then $x' = (x₁', …, x_d')$ with
\begin{align*}
  x_i' =
  \begin{cases}
    \frac{1}{\{x_ℓ\}},  & \text{ if } i = ℓ, \\
    -\frac{\{x_i\}}{\{x_ℓ\}} & \text{ otherwise,}
  \end{cases}
\end{align*}
is the solution in the next iteration.
This update rule follows from the fact that
\[
  B_ℓ \{x_ℓ\} + \sum_{i ≠ ℓ} B_i \{x_i\} = r
  \iff
  r - \sum_{i ≠ ℓ} B_i \{x_i\} = B_ℓ \{x_ℓ\}
  \iff
  r \frac{1}{\{x_ℓ\}} - \sum_{i ≠ ℓ} B_i \frac{\{x_i\}}{\{x_ℓ\}} = B_ℓ.
\]

% TODO: Should we add a citation for Northshield and explain that continued
% fractions map positive to positive values which seems to be a fundamental
% requirement for the continued fractions to be periodic?

% I think a better wording would be, that the update rule makes the negation
% visible, which is not optimal. The update rule itself doesn't negate the
% variables, even without the update rule we would still have negated
% variables, since the update rule is just an improvement of the original
% algorithm.

\begin{figure}[tbp]
  \centering
  \includestandalone{figures/pivot-choice}
  \caption{
    Different choices for the remainder of vector $c$. The original algorithm
    always uses $r$ as the remainder, but the modified update rule would also consider $r'$.}
\end{figure}

\begin{Pseudocode}[
    float=tbp,
    label={lst:modified-generalized-euclidean},
    caption={
      The modified algorithm, where the solution $x$ remains entirely positive in each iterations.
      The linear system from the original algorithm is replaced by the update rule.
      The two negations ensure that the vectors represent the correct solution,
      when the modified update rule from Equation~\ref{eq:modified-update-rule} is used.
    }]
solve $Bx = c$
while $x$ is not integral do
  find $x_ℓ$ which is not integral
  $c ← B_ℓ$
  $B_ℓ ← -B\{x\}$
  $B ← -B$
  $x ← \mathrm{pivot}_ℓ(x)$
end
\end{Pseudocode}

Although the update rule speeds up the algorithm considerably, it is not
optimal for the construction of a generalized continued fraction.
The problem is that the rule flips the sign of all elements inside the solution
vector in each iteration, so it ends up producing negative values.
Instead, I have slightly modified the rule such that it maps each $xᵢ$ to
another positive value.
After we replace $B_ℓ$ with $c$, we flip the signs of all vectors $B_i$ with $i ≠ ℓ$.
This leads to the modified update rule, where the values $x_i$ for $i ≠ ℓ$ are
no longer negated:
\begin{align}
  \label{eq:modified-update-rule}
  x_i' =
  \begin{cases}
    \frac{1}{\{x_ℓ\}},  & \text{ if } i = ℓ, \\
    \frac{\{x_i\}}{\{x_ℓ\}} & \text{ otherwise.}
  \end{cases}
\end{align}
By $\mathrm{pivot}_ℓ(x) = x'$, we denote this modified update rule.
The modified algorithm can be seen in Listing~\ref{lst:modified-generalized-euclidean}.
In the algorithm, first $B_ℓ$ is flipped and then the whole matrix $B$ is flipped,
This is the same as only flipping the vectors $B_i$ for $i ≠ ℓ$.

% ==============================================================================
\section{Comparison to the Jacobi-Perron Algorithm}
% ==============================================================================

When Jacobi \cite{Jacobi68} initially tried answering Hermite's question,
he developed his own generalization of the Euclidean algorithm.
His version computed the GCD of three numbers in a distinctly different way
than the Euclidean algorithm would.
However, he was unable to answer Hermite's question.
Later, Perron \cite{Perron07} extended Jacobi's algorithm to $n$ numbers for any $n ≥ 2$,
which is now known as the Jacobi-Perron algorithm.
He showed that when the algorithm is periodic, then the input consists of
algebraic numbers with degree $n$.
But even with his generalization, he was unable to show the other direction.

\begin{figure}[tbp]
  \centering
  \includestandalone{figures/jacobi-perron}
  \caption{
    One round of the Jacobi-Perron algorithm.
    The smallest number in the input is $7$,
    so it is used as the modulus.
    In the next iteration, $a_5$ is removed since it has remainder zero.
  }
  \label{fig:jacobi-perron}
\end{figure}

The input to the Jacobi-Perron algorithm is a list of numbers $a₁, …, aₙ$ and
it returns the GCD of these numbers.
The algorithm works in multiple rounds.
In each round, the algorithm takes the smallest number $a_ℓ$ and calculates the
remainder $aᵢ' = aᵢ \bmod a_ℓ$ for every $i ≠ ℓ$.
See Figure~\ref{fig:jacobi-perron} for an example of one round.
In the next round, the algorithm continues with the values $a₁', …, aₙ'$ where $a_ℓ' = a_ℓ$.
If any value $aᵢ$ is zero, then we remove the value from the input and continue
with the remaining numbers.
This process continues until all values have a remainder of zero.
The last modulus is the GCD for all numbers of the original input.

% TODO: Explain this better
The next step is extending the algorithm to real numbers,
which can simply be done by allowing real numbers in the input.
In this case, the numbers no longer reach zero, but approach it.
Therefore, we look at the ratios between the inputs starting with the ratios $x₁ = a₁/1, …, xₙ = aₙ/1$.
Given the ratios $x₁, …, x_d$ for the current iteration,
the ratios in the next iteration can be calculated by
\[
  x_i' = \frac{x_i}{x_ℓ}, \qquad x_ℓ' = \frac{1}{x_ℓ}.
\]
This is identical to the pivot operation!
The only difference is that the Jacobi-Perron prescribes the strategy:
We always take the smallest element in each iteration.

So the Jacobi-Perron algorithm is one possible strategy of the generalized
Euclidean algorithm.
This has the unintended benefit that cubic roots proven to be periodic for the
Jacobi-Perron algorithm~\cite{Bernstein64,Bernstein71} are automatically
periodic for the generalized Euclidean algorithm.
On the other hand,
the generalized Euclidean algorithm is more flexible than the Jacobi-Perron
algorithm, since we can choose different indices in each iteration.
This is particularly advantageous since computations for the Jacobi-Perron
seem to indicate that the algorithm is not periodic for some
cases.
Elsner and Hasse \cite{Elsner67} ran different variations of the algorithm on
the input $x = (\sqrt[3]{4}, \sqrt[3]{16})$ and after 80 iterations, they have
found no period.
