\chapter{The Generalized Euclidean Algorithm}
\label{ch:generalized-euclidean}

For a real number, we used the Euclidean algorithm to construct its continued
fraction.
We saw that a continued fraction is periodic if and only if the number used in
its construction is a quadratic irrational.
Naturally, we can ask for a generalization of the Euclidean algorithm,
which can be used to construct higher-dimensional continued fractions
and potentially leads to an answer of Hermite's question.

In this chapter, we analyze one such candidate by Klein and Reuter~\cite{Klein24},
which is a generalization of the Euclidean algorithm for lattices.
While the classical Euclidean algorithm works with integers,
the generalized version works with vectors.
It solves the problem of lattice basis computation,
which can be seen as a multidimensional analogue of computing the greatest
common divisor.

% ==============================================================================
\section{Lattice Basis Computation}
% ==============================================================================

% TODO: Rewrite this section
In a vector space of dimension $d$, consider any set of vectors $A = \{A₁, …, Aₙ\}$ with $n > d$.
We can easily reduce this to a basis $B$ by taking $d$ linearly independent vectors from the original set $A$.
In a lattice, this is not as simple.
Taking $d$ linearly independent vectors from a set $A$ could generate a
different lattice than the original one.
For example, consider the matrix
\[
  A = \begin{pmatrix}
    3 & 0 & 2 \\
    0 & 2 & 1 \\
  \end{pmatrix}.
\]
Taking any two linearly independent columns $A_i, A_j$
produces a different lattice than the all column vectors combined.
The reason is that the missing vector cannot be reached by an
integral combination of the other two vectors $A_i, A_j$.
Instead, we have to find new basis vectors for this lattice.

% TODO: Note about the fact that we only care about one additional vector
% TODO: Shouldn't it be the rank instead of n > d?
The problem of finding a basis for a lattice is known as \emph{lattice basis computation}.
The input for this problem is a matrix $A ∈ ℤ^{d×n}$ with $n > d$ and the goal
is to find a basis which generates the same lattice,
i.e. $\mathcal L(A) = \mathcal L(B)$.
The problem can be seen as a multidimensional analogue of
computing the greatest common divisor from a given set of numbers.
Indeed, we can view a set of numbers $a₁, a₂, …, aₙ ∈ ℤ$ as the basis vectors
of the one-dimensional lattice
\[
  a₁ℤ + a₂ ℤ + \dots + aₙ ℤ.
\]
The greatest common divisor represents a reduced basis in this lattice.

Lattice basis computation is one of the most fundamental problems in lattice theory.
As such, many algorithms require it as a basic subroutine~\cite{Ajtai96,Gentry08}.
Previous approaches for computing such bases either used a variant of the LLL
algorithm~\cite{Lenstra82} or computed the Hermite normal form of the
matrix~\cite{Storjohann96}.
The generalized Euclidean algorithm uses a fundamentally different approach,
which more closely resembles the classical Euclidean algorithm for the
one-dimensional setting.

% ==============================================================================
\section{The Algorithm}
% ==============================================================================

\begin{Pseudocode}[
    float=tb,
    label={lst:generalized-euclidean},
    caption={
      The Generalized Euclidean Algorithm \cite{Klein24},
      which given a basis $B$ and vector $c$
      finds a new basis $B'$ which generates the same lattice.
      It works in two steps: Modulo and exchange.
    }]
solve $Bx = c$
while $x$ is not integral do
  find $x_ℓ$ which is not integral
  // Modulo:
  $r ← B\{x\}$
  // Exchange:
  $c ← B_ℓ$; $B_ℓ ← r$
  solve $Bx = c$
end
\end{Pseudocode}

The algorithm is shown in Listing~\ref{lst:generalized-euclidean}.
In this thesis, we only consider the case where we have one additional vector,
i.e. $A ∈ ℤ^{d×(d+1)}$.
We begin by taking a linearly independent subset $B ∈ ℤ^{d × d}$ of the columns of $A$,
leaving behind one column vector $c ∈ ℤ^d$.
The algorithm calculates a new basis $B'$ such that $\mathcal L(B) = \mathcal L(B ∪ \{c\})$.
Its basic structure consists of two steps:
A \emph{modulo} operation and an \emph{exchange} operation.
The operations are similar to the Euclidean algorithm, where we first compute
the modulo $a \bmod b$ and then swap the two inputs.

First, we look at the modulo operation.
Suppose we have a basis $B ∈ ℤ^{d×d}$ with vectors $B₁, …, B_d$,
any vector $a ∈ ℝ^d$ can easily be represented
as a linear combination $B₁x₁ + \dots + B_d x_d$ of the basis vectors.
However, the linear combination is not necessarily an integral combinations,
so the vector $a$ might not be contained in the lattice $\mathcal L(B)$.
But we can split the vector into an integral part,
which is contained in the lattice,
and a fractional part,
which is contained in the fundamental parallelepiped.
If $c = Bx$, then the vector $z = B \floor{x}$ is a lattice point
and $r = c - z$ is contained in the fundamental parallelepiped.
Thus, we can split $c$ into
\[
  c = B\floor{x} + B\{x\} = z + r.
\]
This is essentially a multidimensional division with remainder operation inside a lattice.
It allows us to define a modulo operation for the algorithm as follows:
\[
  c \pmod{Π(B)} := c - B\floor{B^{-1} a}.
\]
It remains to be shown that applying this modulo operation on $a$ does not
affect the underlying lattice $\mathcal L(B ∪ \{a\})$.

\begin{lemma}
  \label{lem:lattice-mod}
  Let $A = \{A_1, \dots, A_n\}$ with $A_i ∈ ℤ^d$ and let $B$ be a linearly independent subset of $A$.
  If $r = c \pmod{Π(B)}$ for any vector $c ∈ A \setminus B$, then
  \[
    \mathcal L(A) = \mathcal L((A \setminus \{c\}) ∪ \{r\}).
  \]
\end{lemma}

\begin{proof}
  By definition of the modulo operation $c'$ is an integral combination of $c'$ and $B$,
  specifically $c' = c - B \floor{B^{-1} c}$.
  Therefore, $c' ∈ \mathcal L(A)$ and $\mathcal L(A) ⊆ \mathcal L(A \setminus \{c\} ∪ \{c'\})$.
  Similarly, we can reach $c$ by the integral combination $c' + B \floor{B^{-1} c}$,
  so $c ∈ \mathcal L(A \setminus \{c\} ∪ \{c'\})$.
  Therefore applying the modulo operation does not change the lattice.
\end{proof}

With the modulo operation done, we turn our attention to the exchange operation.
On one side we have the basis $B$ and on the other we have the vector $c$.
So a swap of the two inputs will not work.
Instead, we swap $c$ with a single column $B_ℓ$ of the basis $B$,
where the specific index $ℓ$ depends on the result from the modulo operation.
For the modulo operation, we had to solve a linear system $Bx = c$.
If one element $x_i$ is integral, then $c$ is already contained in the
sub-lattice $\mathcal L(B_i)$.
Therefore, it would not make sense to swap with $B_i$.
Instead, we exchange $c$ with any vector $B_ℓ$, where $x_ℓ$ is not integral,
and we swap the vector $B_ℓ$ with the remainder $r = c \, (\mathrm{mod} \, Π(B))$.

This also leads directly to the terminating condition.
The algorithm terminates when the vector $x$ is entirely integral.
In this case, the vector $c$ is an integral combination of all basis vectors,
which means that we have found a new basis for the original lattice.

\begin{lemma}
  \label{lem:termination}
  The algorithm terminates in at most $\det(B)$ iterations.
\end{lemma}

\begin{proof}
  By Cramer's rule, $\{x_ℓ\} = \frac{\det B'}{\det B}$
  and therefore the determinant decreases by a factor of $\{x_ℓ\} < 1$.
  Since the vectors in $B$ and $B'$ are integral, the determinant must also be integral.
  Therefore, the determinant decreases by at least one and in total it
  decreases at most $\det(B)$ iterations.
\end{proof}

In summary, the algorithm requires the following steps:
\begin{itemize}
  \item \textbf{Modulo}: $c \pmod{\Pi(B)} = c - B \floor{B^{-1} c}$.
  \item \textbf{Exchange}: $c = B_ℓ$
  \item \textbf{Termination}: $B^{-1} c ∈ ℤ^d$
\end{itemize}
and the previous two lemmas lead to the following theorem:

\begin{theorem}
  The generalized Euclidean algorithm solves the lattice basis reduction problem.
\end{theorem}

This solves the problem of computing a basis for an integer lattice.
The authors proceed to optimize the algorithm to achieve a better running time.
Of those optimization we need one, which removes the need to solve a linear
system in each iteration.

Instead of solving the system completely from new again,
we simply update the solution from the previous iteration.
Suppose $x = (x₁, …, x_d)$ is the solution of the current iteration,
then we calculate the next vector $x' = (x₁', …, x_d')$ using
\begin{align}
  \label{eq:update}
  x_i' =
  \begin{cases}
    \hphantom{-}\frac{1}{\{x_ℓ\}},  & \text{ if } i = ℓ, \\
    -\frac{\{x_i\}}{\{x_ℓ\}}, & \text{ if } i ≠ ℓ,
  \end{cases}
\end{align}
where $\{x_i\} := x_i - \floor{x_i}$.
This update rule follows from the fact that
\[
  B_ℓ \{x_ℓ\} + \sum_{i ≠ ℓ} B_i \{x_i\} = r
  \Leftrightarrow
  r - \sum_{i ≠ ℓ} B_i \{x_i\} = B_ℓ \{x_ℓ\}
  \Leftrightarrow
  r \frac{1}{\{x_ℓ\}} - \sum_{i ≠ ℓ} B_i \frac{\{x_i\}}{\{x_ℓ\}} = B_ℓ.
\]
With this rule,
we only need to solve the linear system at the start of the algorithm.
Afterwards we use the rule to update our solution vector.
Thus, this optimization reduces the running time significantly.

% ==============================================================================
\section{Constructing Multidimensional Continued Fractions}
% ==============================================================================

The problem of lattice basis reduction only applies to integer lattices.
If we were to solve the problem for a real lattice,
we would have the problem that the generated lattice is no longer a discrete subgroup of $ℝ^n$.
For example, the lattice generated by the one-dimensional vectors $\sqrt{2}$ and $1$
has vectors which are infinitely close.
Thus, this integral combination cannot be considered as a typical lattice.
Nevertheless,
we can still run the generalized Euclidean algorithm on a real input.
We simply allow a matrix $B ∈ ℝ^{d×d}$ and a real vector $c ∈ ℝ^d$.
We use this for the first part of Hermite's question concerning a
representation of real numbers.

Even in a real lattice,
Lemma~\ref{lem:lattice-mod} still holds,
so the vectors are all integral combinations of the initial inputs.
However, the algorithm no longer terminates in $\det(B)$ iterations
as shown in Lemma~\ref{lem:termination}.
The issue is that a lattice generated by
real vectors $B₁, …, B_d, c$ may not have any basis $B' ∈ ℝ^{d×d}$.
In that case, the algorithm runs indefinitely and the determinant approaches
but never reaches zero.

\begin{example}
  Consider the one-dimensional vectors $\sqrt{2}$ and $1$, again.
  There is simply no basis $b ∈ ℝ$,
  which solves $zb = 1$ and $zb = \sqrt{2}$ for some $z ∈ ℤ$ at the same time.
  Otherwise, either $1$ would have to be irrational or $\sqrt{2}$ would have to be rational,
  both of which are absurd.
  Thus, the Euclidean algorithm never terminates on this input.
\end{example}

This is no issue for representing real numbers though.
For the representation,
we need to map each real number to a sequence of integers.
But the algorithm accepts a whole linear system,
not just a single number.
Hence, we need a way of mapping a real number to a linear system.
We do this in two steps:
First, we map the number to a solution vector $x$
and then we find a linear system,
which has $x$ as the solution.
The second part is easy,
since we can choose $B = I_d$ and $c = x$.

For the first part, we just have to choose a vector for every real number.
This can be realized as a function which maps any number $r ∈ ℝ$ to a vector $x ∈ ℝ^d$.
In this thesis, we will use the function
\[
  r ↦ (r, r^2, …, r^d).
\]
% TODO: The reason is that this vector represents the power basis, (at least partially. The first value is missing.)
This function has the advantage that the elements of the vector are linearly independent,
if $r$ is an algebraic number with degree $≥ d$.
However, any function which keeps the elements linearly independent would work.
In general, we can choose polynomials $q_1(x), q_2(x), …, q_d(x)$ with rational
coefficients where $\deg(q_i) = i$ and
use the solution vector $x = (q_1(r), …, q_d(r))$.
But for this thesis, I will use the vector $(r, r^2, …, r^d)$ unless noted otherwise.

% TODO: Rewrite this whole section
% TODO: For the next chapter, we should remark that the entire execution only
% depends on the solution vector, not the original input matrix

% TODO: There should be some explanation on the fact that the solution vector
% kind of represents the ratio between the inputs just like in the Euclidean
% algorithm.

This covers a mapping from a real number to an input for the generalized Euclidean algorithm.
What is missing is the sequence of integers.
In the classical Euclidean algorithm,
we used the ratio $b^{-1}a$ between the two inputs $a, b ∈ ℤ$ to derive the sequence,
which were then used as coefficients in a continued fraction.
At each step, the integer part of $\floor{b^{-1} a}$ determined the specific
coefficient in the representation.
In a sense, we can use the solution vector $x = B^{-1} a$ as a ratio between the inputs.
The integer part $\floor{x}$ now defines a whole vector, which we use as
one part in the integer sequence.
This also gives us a representation of an entire vector not just a single number.
Indeed, if we do not use the mapping, then we can find a representation for any real vector.
Therefore, if $x, x', x'', …$ are the solution vectors at each iteration of the algorithm,
then we represent the original input vector $x$ as a list of integer vectors
$\left[\floor{x}; \floor{x'}, \floor{x''}, …\right]$.

Since we only need the solution vector,
we can use the update rule from Equation~\ref{eq:update}
to calculate the next solution vector.
This removes the entire lattice from the equation
and we only need to focus on the solution vector itself.
This gives us a way to construct \emph{multidimensional continued fractions}.
In fact, for $d = 1$ the update rule matches the definition of continued fractions.

% I think a better wording would be, that the update rule makes the negation
% visible, which is not optimal. The update rule itself doesn't negate the
% variables, even without the update rule we would still have negated
% variables, since the update rule is just an improvement of the original
% algorithm.

\begin{Pseudocode}[
    float=tbp,
    label={lst:modified-generalized-euclidean},
    caption={
      The algorithm for constructing a multidimensional continued fraction
      based on the generalized Euclidean algorithm.
      It constructs a representation for a vector $x ∈ ℝ^d$
      as a list ${[a^{(0)}; a^{(1)}, …]}$ with $a^{(n)} ∈ ℤ^d$
      using the pivot operation from Equation~\ref{eq:modified-update-rule}.
    }]
$n ← 0$
$x^{(0)} ← x$
$a^{(0)} ← \floor{x}$
while $a^{(n)} ≠ 0$ do
  $x^{(n+1)} ← \mathrm{pivot}_ℓ(x^{(n)})$
  $a^{(n+1)} ← \floor{x^{(n+1)}}$
  $n ← n + 1$
end
\end{Pseudocode}

The update rule would suffice for a representation of the real numbers.
However, this representation has some downsides.
The main problem is that the values inside the solution vector $x$ can become negative.
The rule flips the signs of all elements $x_i$ with $i ≠ ℓ$
and can therefore produce negative values.
For continued fractions, it is crucial that the complete quotients are positive \cite{Northshield11}
and for multidimensional continued fractions, this is equally important.
Without it, the structure of the algorithm becomes unstable and convergence may fail.
To avoid negative values, we slightly modify the rule such that it maps each $xᵢ$ to another positive value.
After we replace $B_ℓ$ with $c$, we flip the signs of all vectors $B_i$ with $i ≠ ℓ$.
This leads to the modified update rule, where the values $x_i$ for $i ≠ ℓ$ are
no longer negated:
\begin{align}
  \label{eq:modified-update-rule}
  x_i' =
  \begin{cases}
    \frac{1}{\{x_ℓ\}},  & \text{ if } i = ℓ, \\
    \frac{\{x_i\}}{\{x_ℓ\}} & \text{ otherwise.}
  \end{cases}
\end{align}
By $\mathrm{pivot}_ℓ(x) = x'$, we denote this modified update rule.
We call $x_ℓ$ the \emph{pivot element} and $ℓ$ the \emph{pivot index}.
The final algorithm for constructing MCFs can be seen in
Listing~\ref{lst:modified-generalized-euclidean}.

Unfortunately, this representation is not unique.
But this gives us more flexibility.
To find a unique representation, all we need is a strategy,
that decides which element $x_ℓ$ to choose for pivoting.
For example, the transform of the Jacobi-Perron algorithm,
which is defined as
\begin{align*}
  T(x₁, …, x_d) =
  \left(
  \frac{x_2 - \floor{x_2}}{x_1 - \floor{x_1}},
  \frac{x_3 - \floor{x_3}}{x_1 - \floor{x_1}},
  …,
  \frac{x_d - \floor{x_d}}{x_1 - \floor{x_1}},
  \frac{1}{x_1 - \floor{x_1}}
  \right),
\end{align*}
is one possible strategy.
If we keep the order of the elements
instead of moving $x₁$ to the end,
then the strategy chooses $1$ in the first iteration,
$2$ in the second iteration and so on.
After $d$ iterations, the strategy repeats.

\begin{example}
  Consider the vector $x = (13/5, 8/5)$.
  It has at least two different representations.
  It can be represented as $[(2, 1); (1, 1), (1, 0), (2, 0)]$, since
  \[
    \begin{array}{lclcl}
                                  &   & (13/5, \, 8/3) & = & (2, \, 1) + (3/5, \, 3/5) \\
      \mathrm{pivot}_1(13/5, \, 8/5) & = & (5/3, \,  1)   & = & (1, \, 1) + (1/3, \, 0)   \\
      \mathrm{pivot}_1(5/3, \, 1)    & = & (3/2, \,  0)   & = & (1, \, 0) + (1/2, \, 0)   \\
      \mathrm{pivot}_1(3/2, \, 1)    & = & (2, \,    0)   & = & (2, \, 0) + (0, \, 0),
    \end{array}
  \]
  or it can be represented as $[(2, 1); (1, 1), (0, 1), (0, 2)]$, since
  \[
    \begin{array}{lclcl}
                                  &   & (13/5, \, 8/5) & = & (2, \, 1) + (3/5, \, 3/5) \\
      \mathrm{pivot}_2(13/5, \, 8/5) & = & (1, \, 5/3)    & = & (1, \, 1) + (0, \, 1/3)   \\
      \mathrm{pivot}_2(1, \, 5/3)    & = & (0, \, 3/2)    & = & (0, \, 1) + (0, \, 1/2)   \\
      \mathrm{pivot}_2(0, \, 3/2)    & = & (0, \, 2)      & = & (0, \, 2) + (0, \, 0).
    \end{array}
  \]
  Hence, the representation for $x$ is not unique.
  However, if we choose $ℓ ∈ \{1, …, d\}$ such that $x_ℓ$ is the minimum
  then $x$ is uniquely represented by $\bigl[(2, 1); (1, 1), (0, 1), (0, 2)\bigr]$.
\end{example}

In conclusion,
we represent a vector $x ∈ ℝ^d$ as a sequence of integer vectors by repeatedly
applying the $\mathrm{pivot}$ operation on it either indefinitely or until $x$ is integral.
The index $ℓ$ is given according to some strategy and at each step we store
$\floor{x}$ as one coefficient in the representation for $x$.
We turn this into a representation for a single number $r ∈ ℝ$ by mapping each
number to the vector $(r^1, r^2, …, r^d)$.
This solves the first part of Hermite's question.
In the next chapters, we will discuss the second part of Hermite's question,
whether the representation is periodic if and only if the number is a cubic
irrational, or algebraic number in general.
