\chapter{The Generalized Euclidean Algorithm}

For a real number, we used the Euclidean algorithm to construct its continued
fraction.
We saw that a continued fractions is periodic if and only if the number used in
its construction is a quadratic irrational.
Naturally, we can ask for a generalization of the Euclidean algorithm,
which can be used to construct higher-dimensional continued fractions
and potentially leads to an answer of Hermite's question.

In this chapter, we will analyze one such possible candidate,
which is a generalization of the Euclidean algorithm by Klein and Reuter~\cite{Klein24}.
While the Euclidean algorithm works with integers,
the generalized version works with vectors.
It solves the problem of lattice basis computation,
which can be seen as a multi-dimensional analogue of computing the greatest
common divisor.

% ==============================================================================
\section{Basics of Lattice Theory}
% ==============================================================================

One of the fundamental structures in linear algebra is the vector space.
Given a set of vectors $A = \{A_1, …, A_n\} ∈ ℝ^d$, the span of $A$ is the set
of all linear combinations of the basis vectors $A_i$.
A lattice is similarly defined over a set of vectors $A$, but instead of linear
combinations of vectors, a lattice consists of only integral linear
combinations.
Formally, a lattice is defined as follows:

\begin{definition}
  Given a matrix $A ∈ ℤ^{d × n}$, the \emph{lattice} over the basis $B$ is defined as
  \[
    \mathcal{L}(A) = \left\{\, A₁z₁ + \dots + A_n z_n \mid z_1, \dots, z_n ∈ ℤ^d \,\right\}.
  \]
  The \emph{rank} of $\mathcal{L}(A)$ is $n$ and its \emph{dimension} is $d$.
  If $n = d$, then $\mathcal{L}(A)$ is a \emph{full rank} lattice and $A$ is a \emph{basis}.
\end{definition}

In a vector space, an over-defined set of vectors $A = \{A₁, …, Aₙ\}$ with $n > d$
can be easily reduced to a basis $B$ with exactly $d$ vectors by taking $d$
linearly independent vectors from the set $A$.
In a lattice, it is not as simple.
Taking only $d$ linearly independent vectors produces a different lattice than
the whole set, in general.
For example, consider the matrix
\[
  A = \begin{pmatrix}
    3 & 0 & 2 \\
    0 & 2 & 1 \\
  \end{pmatrix}.
\]
Taking any two columns $A_i, A_j$ of the matrix produces a linearly independent
set, but no set produces the same lattice as the whole matrix.
The missing vector $A_k$ cannot be reached by an integral combination of the
other two vectors $A_i, A_j$.

% TODO: Shouldn't it be the rank instead of n > d?
This problem is typically known as \emph{lattice basis computation}.
The input is a matrix $A ∈ ℤ^{d×n}$ with $n > d$ and the goal is to find a
basis which produces the same lattice, i.e. $\mathcal L(A) = \mathcal L(B)$.
The problem can also be seen as a multi-dimensional analogue of
computing the greatest common divisor of a set of numbers.
Indeed, we can view a set of numbers $a₁, a₂, …, aₙ ∈ ℤ$ as the basis vectors
of the one-dimensional lattice
\[
  a₁ℤ + a₂ ℤ + \dots + aₙ ℤ.
\]
The greatest common divisor represents a reduced basis in this lattice.

Previous approaches for computing such bases either used a variant of the LLL
algorithm~\cite{Lenstra82} or computed the Hermite normal form of the
matrix~\cite{Storjohann96}.
The generalized Euclidean algorithm uses a fundamentally different approach.

\begin{figure}[tbp]
  \centering
  \includestandalone{figures/lattice}
  \caption{
    A two-dimensional lattice $\mathcal L(B)$ with the basis vectors $B_1 = (2,
    1)$ and $B_2 = (1, 3)$. The fundamental parallelepiped $Π(B)$ is colored in
    {\color{cyan}cyan}.
  }
\end{figure}

\begin{itemize}
  \item Applications? \cite{Ajtai96,Gentry08}
\end{itemize}

\begin{definition}
  The \emph{fundamental parallelepiped} of a lattice $\mathcal{L}(B)$ with $B ∈ ℤ^{d × n}$ is defined as
  \[
    Π(B) = \left\{\, B₁ x₁ + \dots + B_n x_n \mid x_1, \dots, x_n ∈ [0, 1) \,\right\}
  \]
\end{definition}

A useful fact about the fundamental parallelepiped of a lattice $\mathcal L(B)$
is that if $B$ is a square integer matrix, then the volume of the
parallelepiped $Π(B)$ and the number of integer points $ℤ^n$ contained in
$Π(B)$ is entirely determined by $\mathrm{det}(B)$, i.e.
\[
  \mathrm{vol}(Π(B)) = |Π(B) ∩ ℤ^n| = |\det(B)|.
\]

% ==============================================================================
\section{Description of the Algorithm}
% ==============================================================================

\begin{Pseudocode}[
    float=tb,
    label={lst:generalized-euclidean},
    caption={The Generalized Euclidean Algorithm \cite{Klein24}.}]
solve $Bx = c$
while $x$ is not integral do
  find $x_ℓ$ which is not integral
  $c ← B_ℓ$
  $B_ℓ ← B\{x\}$
  solve $Bx = c$
end
\end{Pseudocode}

The algorithm is shown in Listing~\ref{lst:generalized-euclidean}.
It takes a matrix $B ∈ ℤ^{d × d}$ and a vector $c ∈ ℤ^d$ as input and finds a
new basis $B'$ such that $\mathcal L(B) = \mathcal L(B ∪ \{c\})$.
Its basic structure consists of two steps:
A modulo operation and an exchange operation.
The operations are similar to the Euclidean algorithm, where we first compute
the modulo $a \bmod b$ and then swap the two inputs.

First, we look at the modulo operation.
Suppose we have a basis $B ∈ ℤ^{d×d}$, any vector $a ∈ ℝ^d$ can be represented
as a linear combination of the basis vectors.
In general, the linear combination will not be an integral combination.
% TODO
In a lattice, each point $a ∈ ℝ^d$ can be represented as a combination of a lattice point $z
∈ \mathcal{L}(B)$ and a point in the fundamental parallelepiped $r ∈ Π(B)$.
Specifically,
\[
  a = z + r.
\]
This is essentially a division with remainder inside a lattice.
It allows us to define a modulo operation on the lattice as follows:
\[
  a \pmod{Π(B)} := a - B\floor{B^{-1} a}.
\]

\begin{lemma}
  Given a set of vectors $A = \{A_1, \dots, A_n\}$ with $A_i ∈ ℤ^d$
  and a linearly independent subset $B$,
  let $r = c \pmod{Π(B)}$ for any vector $c ∈ A \setminus B$.
  Then,
  \[
    \mathcal L(A) = \mathcal L(A \setminus \{c\} ∪ \{r\}).
  \]
\end{lemma}

\begin{proof}
  By definition of the modulo operation $c'$ is an integral combination of $c'$ and $B$,
  specifically $c' = c - B \floor{B^{-1} c}$.
  Therefore, $c' ∈ \mathcal L(A)$ and $\mathcal L(A) ⊆ \mathcal L(A \setminus \{c\} ∪ \{c'\})$.
  Similarly, we can reach $c$ by the integral combination $c' + B \floor{B^{-1} c}$,
  so $c ∈ \mathcal L(A \setminus \{c\} ∪ \{c'\})$.
  Therefore applying the modulo operation does not change the lattice.
\end{proof}

With the modulo operation done, we turn our attention to the exchange operation.
On one side we have the basis $B$ and on the other we have the vector $c$.
So a swap of the two inputs will not work.
Instead, we swap $c$ with a single column $B_ℓ$ of the basis $B$,
where the specific index $ℓ$ depends on the result from the modulo operation.
For the modulo we solve a linear system $Bx = c$.
If one element $x_i$ is integral, then $c$ is already contained in the
sub-lattice $\mathcal L(B_i)$.
In the one-dimensional case, this means that $c$ is a multiple of $B_i$,
so it wouldn't make sense to swap with $B_i$.
Instead, we only exchange $c$ with a vector $B_ℓ$, where $x_ℓ$ is not integral
and we swap $B_ℓ$ with the remainder $c \pmod{Π(B)}$.

The exchange operation also leads directly to the stop condition.
The algorithm terminates when all the vector $x$ is entirely integral.
In this case, the vector $c$ is an integral combination of the basis vectors,
so we can leave it out of the basis.

\begin{lemma}
  The algorithm terminates in at most $\det(B)$ iterations.
\end{lemma}

\begin{proof}
  By Cramer's rule, $\{x_ℓ\} = \frac{\det B'}{\det B}$
  and therefore the determinant decreases by a factor of $\{x_ℓ\} < 1$.
  Since the vectors in $B$ and $B'$ are integral, the determinant must also be integral.
  Therefore, the determinant decreases by at least one and in total it
  decreases at most $\det(B)$ iterations.
\end{proof}

In summary, the algorithm requires the following steps:
\begin{itemize}
  \item \textbf{Modulo}: $c \pmod{\Pi(B)} = c - B \floor{B^{-1} c}$.
  \item \textbf{Exchange}: $c = B_ℓ$
  \item \textbf{Termination}: $B^{-1} c ∈ ℤ^d$
\end{itemize}
and the previous two lemmas lead to the following theorem:

\begin{theorem}
  The generalized Euclidean algorithm solves the lattice basis reduction problem.
\end{theorem}

% ==============================================================================
\section{Extension to Real Numbers}
% ==============================================================================

% TODO: There should be some explanation on the fact that the solution vector
% kind of represents the ratio between the inputs just like in the Euclidean
% algorithm.
The last step needed for the analysis is an extension of the algorithm to real
numbers.
In the original Euclidean algorithm,
we used the ratio $b^{-1}a$ between the two inputs to derive a variant,
which works on real numbers.
In a sense, we use the solution vector $x = B^{-1}a$ as a ratio between the inputs
and we allow $x$ to be a real vector.
In the original lattice reduction problem, this means that we allow the
algorithm to take in real vectors as input.
Just like with the Euclidean algorithm, this causes the algorithm to continue
infinitely as long as the vector $x$ is not rational.

One problem is that calculating the next solution vector requires solving a
linear system.
However, there is a more direct way to calculate the solution vector $x$.
In the original paper, the authors proceed to apply various optimization
to speed up the basic algorithm presented here.
One of those optimizations, removes the need to solve a linear system in each iteration.
Instead of recomputing a solution each time,
we can update the current solution to gain the solution for the next iteration.
If $x = (x₁, …, x_d)$ is the solution in the current iteration,
then $x' = (x₁', …, x_d')$ with
\begin{align*}
  x_i' =
  \begin{cases}
    \frac{1}{\{x_ℓ\}},  & \text{ if } i = ℓ, \\
    -\frac{\{x_i\}}{\{x_ℓ\}} & \text{ otherwise,}
  \end{cases}
\end{align*}
is the solution in the next iteration.
This update rule follows from the fact that
\[
  B_ℓ \{x_ℓ\} + \sum_{i ≠ ℓ} B_i \{x_i\} = r
  \iff
  r - \sum_{i ≠ ℓ} B_i \{x_i\} = B_ℓ \{x_ℓ\}
  \iff
  r \frac{1}{\{x_ℓ\}} - \sum_{i ≠ ℓ} B_i \frac{\{x_i\}}{\{x_ℓ\}} = B_ℓ.
\]

% TODO: Should we add a citation for Northshield and explain that continued
% fractions map positive to positive values which seems to be a fundamental
% requirement for the continued fractions to be periodic?

% I think a better wording would be, that the update rule makes the negation
% visible, which is not optimal. The update rule itself doesn't negate the
% variables, even without the update rule we would still have negated
% variables, since the update rule is just an improvement of the original
% algorithm.

Although the update rule speeds up the algorithm considerably, it is not
optimal for the construction of a generalized continued fraction.
The problem is that the rule flips the sign of all elements inside the solution
vector in each iteration, so it ends up producing negative values.
Instead, I have slightly modified the rule such that it maps each $xᵢ$ to
another positive value.
After we replace $B_ℓ$ with $c$, we flip the signs of all vectors $B_i$ with $i ≠ ℓ$.
This leads to the modified update rule, where the values $x_i$ for $i ≠ ℓ$ are
no longer negated:
\begin{align*}
  x_i' =
  \begin{cases}
    \frac{1}{\{x_ℓ\}},  & \text{ if } i = ℓ, \\
    \frac{\{x_i\}}{\{x_ℓ\}} & \text{ otherwise.}
  \end{cases}
\end{align*}
By $\mathrm{pivot}_ℓ(x) = x'$, we denote this modified update rule.
The modified algorithm can be seen in Listing~\ref{lst:modified-generalized-euclidean}.
In the algorithm, first $B_ℓ$ is flipped and then the whole matrix $B$ is flipped,
This is the same as only flipping the vectors $B_i$ for $i ≠ ℓ$.

\begin{Pseudocode}[float=tb, caption={The Modified Algorithm.}, label={lst:modified-generalized-euclidean}]
solve $Bx = c$
while $x$ is not integral do
  find index $ℓ$ for which $x_ℓ$ is not integral
  $c ← B_ℓ$
  $B_ℓ ← -B\{x\}$
  $B ← -B$
  $x ← \mathrm{pivot}_ℓ(x)$
end
\end{Pseudocode}

\begin{figure}[t]
  \centering
  \includestandalone{figures/pivot-choice}
  \caption{
    Different choices for the remainder of vector $c$. The original algorithm
    always uses $r$ as the remainder, but the modified update rule would also consider $r'$.}
\end{figure}

% TODO: Proof that the solution vector converges to zero?

% TODO: Example for some cubic roots?
\begin{example}

\end{example}

% ==============================================================================
\section{Comparison to the Jacobi-Perron Algorithm}
% ==============================================================================

Many generalizations to the Euclidean algorithm have been considered.
One of them was proposed by Jacobi to answer Hermite's question.
In his version, he computes the GCD of three numbers by successively dividing
the smallest number from the larger numbers.
This algorithm was later extended by Oskar Perron to arbitrarily many numbers.
The algorithm works as follows:

Given a list of positive integers $a₀, a₁, …, aₙ$, take the smallest number $a_ℓ$
and compute the remainder $a_i'$ resulting from the division of $a_i$ with $a_ℓ$.
The value $a_ℓ$ is kept until the next iteration, i.e. $a_ℓ' = a_ℓ$.
Continue this process until all but one value remains.
\begin{align*}
  a₀' = a₀ \bmod a_ℓ, a₁ = a₁ \bmod a_ℓ, …, a_ℓ' = a_ℓ, …, aₙ' = aₙ \bmod a_ℓ; \\
\end{align*}

Perron modified this algorithm for the purposes of his analysis.
The integers $a₁, …, aₙ$ are kept in a list.
We remove the first element from the list, calculate the remainders for each
remaining element and append the element to the end of the list.
One iteration in this modified version produces the values:
\begin{align*}
  a₀' = a₁ \bmod a₀, a₁' = a₂ \bmod a₀, …, a_{n-1}' = a_n \bmod a₀, aₙ = a₀. \\
\end{align*}
This process is repeated until the first element is zero.

Of course, the termination condition is not sufficient.
When this algorithm terminates, the remaining elements might not all be zero.
Therefore, we remove the first element from the list and continue with the
remaining list.

By allowing real numbers as inputs, this algorithm proceeds infinitely but
always converges to zero.

The Jacobi-Perron algorithm is actually a subset of the generalized Euclidean algorithm.
The generalized Euclidean algorithm is periodic for all real numbers where the Jacobi-Perron algorithm is periodic.
This comes from the fact, that the Jacobi-Perron algorithm is really the
generalized Euclidean algorithm with the specific sequence of pivots $L = \overline{12…d}$.
So in the $i$th iteration, we are choosing index $(i \bmod d) + 1$ as our pivot.

This has the convenient property that if the all inputs which are periodic for
the Jacobi-Perron algorithm must also be periodic for the brute-force algorithm.

\begin{theorem}
  The brute-force algorithm is periodic on input $(1, r, r^2)$ if
  \[
    r = \sqrt[n]{D + d}, \text{ where } d | D.
  \]
\end{theorem}
